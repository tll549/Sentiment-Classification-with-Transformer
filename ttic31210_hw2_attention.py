# -*- coding: utf-8 -*-
"""TTIC31210_HW2_Attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bZR2M7r3su8ddEcWZy7riiDUagiDSOzs

# TTIC 31210 HW2 Attention

- Name: Yi Hung Liu
- UCID: yhliu
- Email: yhliu@uchicago.edu


---


For reading convenience, I suggest read this on online [Colab notebook](https://colab.research.google.com/drive/1bZR2M7r3su8ddEcWZy7riiDUagiDSOzs) and fold all the "Code" and "Printout" section so that only essential results are displayed. Or use the table of contents sidebar (View -> Table of contents).

# 1 Word Averaging Binary Classifier

## 1.1 Implementation and experimentation

### 1.1.1 Code
"""

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import time
import matplotlib.pyplot as plt
import gensim.corpora as corpora
import copy
import heapq
import math

# load data
FROM_GOOGLE_DRIVE = True
if FROM_GOOGLE_DRIVE:
    from google.colab import drive
    drive.mount('/content/drive')
    DRIVE_DIR = '/content/drive/My Drive/'
else:
    DRIVE_DIR = '/Users/T/Google Drive/'
WD = DRIVE_DIR + '02 UChicago/3 19Spring/TTIC 31210 Advanced NLP/HW2/'

def load_data(name, from_google_drive = True):
    data = np.genfromtxt(WD + '31210-s19-hw2/' + name, dtype = np.dtype(str), delimiter = "\n", comments = None) # use this instead of np.loadtxt
    x = [d.split()[:-1] for d in data] # ignore x[-1] = y
    y = [int(d[-1]) for d in data]
    print(name, len(x))
    return(x, y)
    
x_train, y_train = load_data('senti.train.tsv')
x_dev, y_dev = load_data('senti.dev.tsv')
x_test, y_test = load_data('senti.test.tsv')

# generate dic  
dic = corpora.Dictionary(x_train + x_dev + x_test)
print(dic)

# helper functions
def tensorize_sen(sen, gpu):
    x = dic.doc2idx(sen)
    tens = torch.tensor(x)
    if gpu:
        tens = tens.cuda()
    return(tens)

def plot_epoch_losses_acc(epoch_train_losses, epoch_train_acc, epoch_test_losses, epoch_test_acc):
    plt.figure(figsize = (6, 2))
    plt.subplot(121)
    plt.plot(range(len(epoch_train_losses)), epoch_train_losses)
    plt.plot(range(len(epoch_test_losses)), epoch_test_losses)
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(('train', 'dev'))
    plt.title('Loss by Epoch')

    plt.subplot(122)
    plt.plot(range(len(epoch_train_acc)), epoch_train_acc)
    plt.plot(range(len(epoch_test_acc)), epoch_test_acc)
    plt.ylabel('Acc')
    plt.xlabel('Epoch')
    plt.legend(('train', 'dev'))
    plt.title('Accuracy by Epoch')
    plt.show()
    return(0)

def create_pos(x, embedded, embedding_dim):
    '''create positional embedding matrix'''
    m = torch.zeros(len(x), embedding_dim)
    for pos in range(len(x)):
        for i in range(0, embedding_dim-1, 2):
            m[pos, i] = math.sin(pos / (10000 ** ((2 * i)/embedding_dim)))
            m[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/embedding_dim)))
    
    if embedded.is_cuda:
        m = m.cuda()
        
    # make embeddings relatively larger
    embedded = embedded * math.sqrt(embedding_dim)
    # add constant to embedding
    embedded = embedded + m
    return(m)

# main transformer
class Attention(nn.Module):
    def __init__(self, vocab_size, embedding_dim, att_type, att_dim = None, num_head = None):
        super(Attention, self).__init__()
#         self.emb = nn.Embedding(vocab_size, embedding_dim)
        self.emb = nn.Embedding(vocab_size, embedding_dim, sparse = True)
        self.w = nn.Linear(embedding_dim, 1, bias = False)
        
        # for attention weighted word averaging
        if att_type == 'att_weighted':
            self.cos = nn.CosineSimilarity(dim = 1)
            self.u = nn.Parameter(torch.randn(1, embedding_dim, dtype = torch.float)) # 1*D
        
        # different query, key, value matrix
        elif att_type == 'self_att':
            if att_dim is None:
                self.att_dim = embedding_dim
            else:
                self.att_dim = att_dim
            
            self.w_v = nn.Parameter(torch.randn(embedding_dim, self.att_dim, dtype = torch.float))
            self.w_k = nn.Parameter(torch.randn(embedding_dim, self.att_dim, dtype = torch.float))
            self.w_q = nn.Parameter(torch.randn(embedding_dim, self.att_dim, dtype = torch.float))
            self.w2 = nn.Linear(self.att_dim, 1, bias = False) # same usage as self.w but with different dim
        
        elif att_type == 'self_att_multi_head_sum' or att_type == 'self_att_multi_head_cat' or att_type == 'self_att_multi_head_cat_pos':
            if att_type == 'self_att_multi_head_cat' or att_type == 'self_att_multi_head_cat_pos':
                self.att_dim = int(round(embedding_dim / num_head)) # for using concatenate below, e.g. 100/5 = 20, 512/8 = 64
            elif att_type == 'self_att_multi_head_sum':
                self.att_dim = embedding_dim # for using sum below instead of concatenate
            self.num_head = num_head
        
            self.w_v = nn.Parameter(torch.randn(num_head, embedding_dim, self.att_dim, dtype = torch.float))
            self.w_k = nn.Parameter(torch.randn(num_head, embedding_dim, self.att_dim, dtype = torch.float))
            self.w_q = nn.Parameter(torch.randn(num_head, embedding_dim, self.att_dim, dtype = torch.float))
            if att_type == 'self_att_multi_head_cat' or att_type == 'self_att_multi_head_cat_pos':   
                self.w_o = nn.Linear(num_head * self.att_dim, 1, bias = False)
            elif att_type == 'self_att_multi_head_sum':
                self.w_o = nn.Linear(self.att_dim, 1, bias = False)

    def forward(self, x, att_type):
        '''
        x: idx for every word, dim = sen_len
        att_type: 
          1. None, 2. att_weighted, 
          3-1. simple_self_att, 3-2.simple_self_att_res_con
          4-1. self_att
          4-2. self_att_multi_head_sum, 4-3. self_att_multi_head_cat
          
        return: scalar
        
        L: sen_len
        D: embedding_dim
        H: num_head
        A: att_dim
        '''
        if att_type == None:
            embedded = self.emb(x) # L -> L*D
            h_avg = torch.mean(embedded, dim = 0) # L*D -> D
            w_h = self.w(h_avg) # wT*h_avg: 1*D * D*1 -> scalar
            out = torch.sigmoid(w_h) # scalar in R -> scalar in (0, 1)
            return(out)
        
        elif att_type == 'att_weighted':
            # alpha = exp(cos(u, emb(x)))
            embedded = self.emb(x) # L -> L*D
            temp = self.cos(self.u, embedded) # cos(1*D, L*D) -> L
            alpha = torch.exp(temp) # L -> L
            alpha = F.normalize(alpha, p = 1, dim = 0) # L -> L
            
            # h_att = sum(alpha * emb(x))
            temp = alpha * embedded.t() # L * (L*D)T -> D*L
            temp = temp.t() # D*L -> L*D
            h_att = torch.sum(temp, dim = 0) # L*D -> D

            # sigmoid(wT * h_att)
            w_h = self.w(h_att) # wT*h_att: D * D -> scalar
            out = torch.sigmoid(w_h) # scalar in R -> scalar in (0, 1)
            return(out)
        
        elif att_type == 'simple_self_att' or att_type == 'simple_self_att_res_con' :
            # a_ts = emb(x_t)T * emb(x_s)
            embedded = self.emb(x) # L -> L*D
            a_ts = torch.matmul(embedded, embedded.t()) # L*D * D*L -> L*L, should be symmetric

            # alpha_t = exp(sum(a_ts))
            temp = torch.sum(a_ts, dim = 1) # L*L -> L
            alpha = F.softmax(temp, dim = 0) # L -> L

            # h_self = sum(alpha * emb(x))
            temp = (alpha * embedded.t()).t() # L * D*L -> L*D
            h_self = torch.sum(temp, dim = 0) # L*D -> D

            # with or without residual connection
            if att_type == 'simple_self_att_res_con':
                # sigmoid(wT * (h_self + h_avg))
                h_avg = torch.sum(embedded, dim = 0)/len(x) # L*D -> D
                out = torch.sigmoid(self.w(h_self + h_avg))
            elif att_type == 'simple_self_att':
                # sigmoid(wT * h_self)
                out = torch.sigmoid(self.w(h_self)) # D * D -> scalar -> scalar
            return(out)
        
        elif att_type == 'self_att':
            '''should be equivalent to one head below'''
            embedded = self.emb(x) # L -> L*D
            # W^(v) * emb(x)
            XWV = torch.matmul(embedded, self.w_k) # L*D * D*A -> L*A
            # W^(k) * emb(x)
            XWK = torch.matmul(embedded, self.w_k) # L*D * D*A -> L*A
            # W^(q) * emb(x)
            XWQ = torch.matmul(embedded, self.w_q) # L*D * D*A -> L*A
            
            # alpha = exp(W^(q) * W^(k))
            temp = torch.matmul(XWQ, XWK.t()) # L*A * A*L -> L*L
            temp = temp / np.sqrt(self.att_dim)
            alpha = F.softmax(temp, dim = 0) # symmetric so doesn't matter?
            
            # alpha * W^(v)
            temp = torch.matmul(alpha, XWV) # L*L * L*A -> L*A
            f = torch.sum(temp, dim = 0) # L*A -> A, like weighted avg

            # sigmoid(wT * f), f is the same meaning as h in hw description
            temp = self.w2(f) # A -> scalar
            out = torch.sigmoid(temp)
            return(out)
        
        elif att_type == 'self_att_multi_head_sum' or att_type == 'self_att_multi_head_cat' or att_type == 'self_att_multi_head_cat_pos':
            embedded = self.emb(x) # L -> L*D
    
            # positional encoding
            if att_type == 'self_att_multi_head_cat_pos':
                embedded = create_pos(x, embedded, embedded.shape[1])
            
            # W^(v) * emb(x)
            XWV = torch.matmul(embedded, self.w_k) # L*D * H*D*A -> H*L*A
            # W^(k) * emb(x)
            XWK = torch.matmul(embedded, self.w_k) # L*D * H*D*A -> H*L*A
            # W^(q) * emb(x)
            XWQ = torch.matmul(embedded, self.w_q) # L*D * H*D*A -> H*L*A
            
            # alpha = exp(W^(q) * W^(k))
            temp = torch.matmul(XWQ, XWK.view(self.num_head, self.att_dim, -1)) # H*L*A * H*A*L -> H*L*L
            temp = temp / np.sqrt(self.att_dim)
            alpha = F.softmax(temp, dim = 0) #?
        
            # alpha * W^(v)
            temp = torch.matmul(alpha, XWV) # H*L*L * H*L*A -> H*L*A
            temp = torch.sum(temp, dim = 1) # H*L*A -> H*A
            if att_type == 'self_att_multi_head_cat' or att_type == 'self_att_multi_head_cat_pos':
                f = temp.view(-1) # H*A -> (H*A), concatenate, method from http://nlp.seas.harvard.edu/2018/04/03/attention.html
            elif att_type == 'self_att_multi_head_sum':
                f = torch.sum(temp, dim = 0) # H*A -> A, sum, method from PPT

            # sigmoid(wT * f), f is the same meaning as h in hw description
            if att_type == 'self_att_multi_head_cat' or att_type == 'self_att_multi_head_cat_pos':
                temp = self.w_o(f) # (H*A) -> scalar, if concatenate above
            elif att_type == 'self_att_multi_head_sum':
                temp = self.w_o(f) # A -> scalar, if sum above
            out = torch.sigmoid(temp)
            return(out)
    
    def get_emb(self):
        return(self.emb.weight)
    
    def get_u(self):
        return(self.u)
    
# create_pos(x_train[0], 5)

# # model = Attention(len(dic), 100, 'simple_self_att', 64)
# model = Attention(len(dic), 100, 'self_att_multi_head_sum', num_head = 5)
# model = model.cuda()
# # att_prob = Att_Prob(selected_words)
# x = tensorize_sen(x_train[0], True)
# # out = model(x, 'simple_self_att')
# out = model(x, 'self_att_multi_head_sum')
    
# training functions
def feed(x_data, y_data, att_type, back_prop, gpu):
    loss_one_epoch, acc_one_epoch = 0, 0
    for i in range(len(x_data)):
        model.zero_grad()
        x = tensorize_sen(x_data[i], gpu)
        
        out_tens = model(x, att_type)

        y_tens = torch.tensor([y_data[i]], dtype = torch.float)
        if gpu:
            y_tens = y_tens.cuda()
        loss = loss_function(out_tens, y_tens)
        
        if back_prop:
            loss.backward()
            optimizer.step()

        loss_one_epoch += loss.item()
        acc_one_epoch += 1 if int(out_tens > 0.5) == y_data[i] else 0
    return(loss_one_epoch/len(x_data), acc_one_epoch/len(x_data))

def eval(train_size, dev_size, test_size, att_type, emb_dim, lr, epochs, 
         try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
         att_dim = None, num_head = None):
    # initialize everything
    global model, loss_function, optimizer, using_gpu
    if seed != None:
        torch.manual_seed(seed)    
    model = Attention(len(dic), emb_dim, att_type, att_dim, num_head)
    using_gpu = try_gpu and torch.cuda.is_available()
    if using_gpu:
        model = model.cuda()
    print('start training...  using GPU:', using_gpu, '\n')

    loss_function = nn.BCELoss()
#     optimizer = optim.Adam(model.parameters(), lr = lr)
#     optimizer = optim.SGD(model.parameters(), lr = lr)
    optimizer = optim.Adagrad(model.parameters(), lr = lr)
#     optimizer = optim.SparseAdam(model.parameters(), lr = lr)
    
    loss_by_epoch_train, acc_by_epoch_train = [], []
    loss_by_epoch_dev, acc_by_epoch_dev = [], []
    model_by_epoch = []
    
    start = time.time()
    for e in range(epochs):
        # train
        l, a = feed(x_train[0:train_size], y_train[0:train_size], att_type, back_prop = True, gpu = using_gpu)        
        loss_by_epoch_train.append(l)
        acc_by_epoch_train.append(a)
        
        model_by_epoch.append(copy.deepcopy(model.state_dict()))
        
        # dev
        l, a = feed(x_dev[0:dev_size], y_dev[0:dev_size], att_type, back_prop = False, gpu = using_gpu)
        loss_by_epoch_dev.append(l)
        acc_by_epoch_dev.append(a)
        
        if verbose:
            if e == 0:
                print('Epoch|TrainLoss    Acc|DevLoss    Acc|RunTime')
            print('{:>5}|{:9.3f}  {:3.3f}|{:7.3f}  {:3.3f}|{:7.1f}'
                  .format(e, loss_by_epoch_train[-1], acc_by_epoch_train[-1], 
                          loss_by_epoch_dev[-1], acc_by_epoch_dev[-1], time.time()-start))
           
    # find and load the best model back
    best_epoch = np.argmax(acc_by_epoch_dev)
    model.load_state_dict(model_by_epoch[best_epoch])
    print(f'best num epoch: {best_epoch + 1} (e = {best_epoch})')
    
    # save the best model to disk
    if save_best:
        hpars = f'train_size-{train_size}.dev_size-{dev_size}.test_size-{test_size}.att_type-{att_type}.emb_dim-{emb_dim}.lr-{lr}.epochs-{epochs}.seed-{seed}.best_num_epoch-{best_epoch+1}'
        torch.save(model.state_dict(), WD + 'trained_model/' + hpars)
        print(f'best model saved to disk as {hpars}')
    
    if plot:
        plot_epoch_losses_acc(loss_by_epoch_train, acc_by_epoch_train, loss_by_epoch_dev, acc_by_epoch_dev)
 
    # test the best model on test set
    l, acc_test = feed(x_test[0:test_size], y_test[0:test_size], att_type, back_prop = False, gpu = using_gpu)
    print(f'\ndev acc: {acc_by_epoch_dev[best_epoch]:.3f}, test acc: {acc_test:.3f}')           
    return(model)

# m = eval(train_size = 101, dev_size = 101, test_size = 101, # max: 67349, 872, 1821
#      att_type = 'self_att_multi_head_cat', emb_dim = 100, lr = 1e-2, epochs = 5,
#      try_gpu = True, verbose = True, plot = True, save_best = False, seed = 555,
#      num_head = 5)
# m = eval(train_size = 101, dev_size = 101, test_size = 101, # max: 67349, 872, 1821
#      att_type = 'self_att_multi_head_cat_pos', emb_dim = 100, lr = 1e-2, epochs = 5,
#      try_gpu = True, verbose = True, plot = True, save_best = False, seed = 555,
#      num_head = 5)

"""### 1.1.2 Final model

$$\mathbf{h}_{a v g}=\frac{1}{|\boldsymbol{x}|} \sum_{t} e m b\left(x_{t}\right)$$

$$\sigma\left(\mathbf{w}^{\top} \mathbf{h}_{a v g}\right)$$
"""

MODEL_NONE = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
                  att_type = None, emb_dim = 100, lr = 5e-4, epochs = 5)

"""After experimentations (see appendix for verbose printout), I use Adam with `lr = 5e-4` and keep other hyperparameters as default. Achieve 81.3% accuracy on `test` and 82% accuracy on `dev`.

## 1.2 Analysis
"""

emb_matrix = MODEL_NONE.get_emb()
norms = torch.norm(emb_matrix, dim = 1)
sorted_idx = sorted(range(len(norms)), key = lambda k: norms[k], reverse = True)
biggest = ''
smallest = ''
for i in range(15):
    biggest += dic[sorted_idx[i]] + ' '
    smallest += dic[sorted_idx[-(i+1)]] + ' '
print('biggest norm in decreasing order (big to small):')
print(biggest)
print('smallest norm in increasing order (small to big):')
print(smallest) # smallest to biggest

"""The words with largest norms are those with strong positive sentiments and some of those with small norms are with negative sentiments.

# 2 Attention-Weighted Word Averaging

$$\begin{array}{c}{\alpha_{t} \propto \exp \left\{\cos \left(\mathbf{u}, e m b\left(x_{t}\right)\right)\right\}} \\ {\mathbf{h}_{a t t}=\sum_{t} \alpha_{t} e m b\left(x_{t}\right)}\end{array}$$

$$\sigma\left(\mathbf{w}^{\top} \mathbf{h}_{a t t}\right)$$

## 2.1 Result

All necessary code are implemented above in 1.1.1, inside the `Attention` class, wich can be accessed by using `att_type = 'att_weighted'`
"""

MODEL_ATT_WEIGHTED = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
                 att_type = 'att_weighted', emb_dim = 100, lr = 5e-4, epochs = 5, seed = 123)

"""The accuracy is 81.7% on `test` and 81.9% on `dev`, which is close to 81.3% from the first part.

## 2.2 Analysis: word embeddings and the attention vector
"""

u = MODEL_ATT_WEIGHTED.get_u()
emb_matrix = MODEL_ATT_WEIGHTED.get_emb()
cos = nn.CosineSimilarity(dim = 1)
cos_result = cos(u, emb_matrix) # 1*100, 19423*100 -> 19423

sorted_idx = sorted(range(len(cos_result)), key = lambda k: cos_result[k], reverse = True)
biggest = ''
smallest = ''
for i in range(15):
    biggest += dic[sorted_idx[i]] + ' '
    smallest += dic[sorted_idx[-(i+1)]] + ' '
print('highest cosine similarity in decreasing order (big to small):')
print(biggest)
print('lowest cosine similarity in increasing order (small to big):')
print(smallest) # smallest to biggest

u = MODEL_ATT_WEIGHTED.get_u()
emb_matrix = MODEL_ATT_WEIGHTED.get_emb()
cos = nn.CosineSimilarity(dim = 1)
cos_result = cos(u, emb_matrix) # 1*100, 19423*100 -> 19423

sorted_idx = sorted(range(len(cos_result)), key = lambda k: cos_result[k], reverse = True)
biggest = ''
smallest = ''
for i in range(15):
    biggest += dic[sorted_idx[i]] + ' '
    smallest += dic[sorted_idx[-(i+1)]] + ' '
print('highest cosine similarity in decreasing order (big to small):')
print(biggest)
print('lowest cosine similarity in increasing order (small to big):')
print(smallest) # smallest to biggest

"""I keep two results from two trainings with different seeds above. For those words with higherst cosine similarity, they seems to have negative sentiment (e.g. not, worse, pointless, nowhere, vague, mess, none, devoid, unfunny, ugly, ...), while those with lowest cosine similarity have positive sentiment (e.g. funny, right, interesting, funny, goodness, great, ...).

## 2.3 Analysis: variance of attentions
"""

dic_train = corpora.Dictionary(x_train)
print(f'number of unique words in train: {len(dic_train)}')
selected_words = [k for k, v in dic_train.dfs.items() if v >= 100]
print(f'number of words appears at least 100 times in train: {len(selected_words)}')

# check the id in dic_train is the same as dic
for i in selected_words:
    if i != dic.token2id[dic_train[i]]:
        raise('id are different')

# calc every alpha
alpha_dic = {i:[] for i in selected_words}
for i in range(len(x_train)):
    x = tensorize_sen(x_train[i], False) # L
    embedded = emb_matrix[x, :] # L*D
    temp = cos(u, embedded) # cos(1*D, L*D) -> L
    alpha = torch.exp(temp) # L -> L
    alpha = F.normalize(alpha, p = 1, dim = 0) # L -> L
    
    # record only frequent words
    for j in range(len(x)):
        if int(x[j]) in alpha_dic.keys():
            alpha_dic[int(x[j])].append(float(alpha[j]))
            
# calc stat and sort
alpha_stat = {i:np.std(l) / np.mean(l) for i, l in alpha_dic.items() if len(l) != 0}
k_large = heapq.nlargest(30, alpha_stat, key = alpha_stat.get) # get k largest keys in dict by value

# print result
for i in range(len(k_large)):
    print(dic[k_large[i]], end = ' ')
    if (i+1) % 6 == 0: 
        print()

"""They seem to be mostly adjective or with multisense, where it can be used in different context so that it may need to have different attention among all kinds of context. For example, 'true' can be used as 'true happiness' or 'this is true', where in the former case it shouldn't have too much attention but in the latter it should.

# 3 Simple self-attention

$$\begin{array}{c}{a_{t s}=e m b\left(x_{t}\right)^{\top} e m b\left(x_{s}\right)} \\ {\alpha_{t} \propto \exp \left\{\sum_{s} a_{t s}\right\}} \\ {\mathbf{h}_{s e l f}=\sum_{t} \alpha_{t} e m b\left(x_{t}\right)}\end{array}$$

$$\sigma\left(\mathbf{w}^{\top} \mathbf{h}_{s e l f}\right)$$
or
$$\sigma\left(\mathbf{w}^{\top}\left(\mathbf{h}_{s e l f}+\mathbf{h}_{a v g}\right)\right)$$

All necessary code are implemented above in 1.1.1, inside the `Attention` class, wich can be accessed by using `att_type = 'simple_self_att'`

## 3.1 Without residual connection
"""

MODEL_SIMPLE_SELF_ATT = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
                         att_type = 'simple_self_att', emb_dim = 100, lr = 5e-4, epochs = 10)

"""## 3.2 With residual connection"""

MODEL_SIMPLE_SELF_ATT_RES_CON = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
                        att_type = 'simple_self_att_res_con', emb_dim = 100, lr = 5e-4, epochs = 10)

"""The result is 0.805 for without residual connection and 0.800 with residual connection. Also close to the above results.

# 4 Enriching 
    
$$\mathbf{f}_{\mathrm{att}}(\boldsymbol{x})=\sum_{i=1}^{n} \sum_{j=1}^{J} a t t_{j}\left(x_{i}, i, \boldsymbol{x}\right)\left(\mathbf{W}_{j}^{(v)} \operatorname{emb}\left(x_{i}\right)\right)$$

$$a t t_{j}\left(x_{i}, i, \boldsymbol{x}\right) \propto \exp \left\{\mathbf{W}_{j}^{(q) \top}\left(\mathbf{W}_{j}^{(k)} \operatorname{emb}\left(x_{i}\right)\right)\right\}$$

$$\text{output} = \sigma(\mathbf{W}^{(o) T} f_{att}(\boldsymbol{x}))$$

All necessary code are implemented above in 1.1.1, inside the `Attention` class, wich can be accessed with the `att_type` argument for `forward` function.

## 4.2 Final model
"""

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat', emb_dim = 100, lr = 5e-4, epochs = 20,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     num_head = 5)

"""## 4.3 Summary

I've tried

- `att_type = 'self_att'`: self-attention with **different transformation matices for query, key and value**
- `att_type == 'self_att_multi_head_sum'` or `att_type == 'self_att_multi_head_cat'`: same as above with **multi-head self-attention** with different number of heads. Using either sum or concatenate for the output from multi-head.
- `att_type == 'self_att_multi_head_cat_pos'`: same as above with **positional encoding**.

All of the results are not better than 80%. They can be froun in the appendix 'enriching results'.

The best result I got is `att_type == 'self_att_multi_head_cat`, using self attention with different transformation matrices for query ($W^{(q)}$), key ($W^{(k)}$), and value ($W^{(v)}$) with multi-heads ($H = 5$), their dimensions are 20*100 where 20 is calculated from the embedding dimentions divided by number of heads ($A = 100 / 5 = 20$). I concatenate the result after self-attention ($f_{att}(x)$) to produce similar dimensions as above different types of attention. The embedding dimension is the same as above ($D = 100$).

I found the best model to be $H = 5$ where the performance is very similar to the first word-averaging model (acc = 0.795 for here compared to acc = 0.813).

# Appendix

Just some of my training printout that I want to keep them here.

References

* [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
* [The Illustrated Transformer – Jay Alammar – Visualizing machine learning one concept at a time](http://jalammar.github.io/illustrated-transformer/)
* [Learn ELMo for Extracting Features from Text (using Python)](https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/)
* [A Beginner's Guide to Attention Mechanisms and Memory Networks | Skymind](https://skymind.ai/wiki/attention-mechanism-memory-network)
* [Translation with a Sequence to Sequence Network and Attention — PyTorch Tutorials 1.1.0.dev20190425 documentation](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#the-encoder)
* [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html)
* [A Brief Overview of Attention Mechanism – SyncedReview – Medium](https://medium.com/syncedreview/a-brief-overview-of-attention-mechanism-13c578ba9129)
* [finding top k largest keys in a dictionary python - Stack Overflow](https://stackoverflow.com/questions/12266617/finding-top-k-largest-keys-in-a-dictionary-python)
"""

MODEL_NONE = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
                  att_type = None, emb_dim = 100, lr = 5e-5, epochs = 20)

eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
     emb_dim = 100, lr = 1e-3, epochs = 5, 
     gpu = True, plot = True, verbose = True, seed = 554)

eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
     emb_dim = 100, lr = 5e-4, epochs = 10, 
     gpu = True, plot = True, verbose = True, seed = 554)

MODEL_NONE = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
                  att_type = None, emb_dim = 100, lr = 1e-5, epochs = 20)

MODEL_NONE = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
                  att_type = None, emb_dim = 100, lr = 1e-4, epochs = 10)

"""With above experimentations, the results for lr, converged epoch, test acc are

- 1e-3, 5?, 0.805
- 5e-4, 10?, 0.812
- 1e-4, 10?, 0.818
- 5e-5, 16, 0.807
- 1e-5, 20, 0.751

I choose to use lr = 5e-4
"""

# diff att_dim
# 32 0.624
# 64 0.653
# 100 0.624

# diff num_head
# 1 .535
# 3 .515
# 5 .554
# 10 .525

MODEL_ATT = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
                 att_type = 'att_weighted', emb_dim = 100, lr = 5e-4, epochs = 5)

MODEL_SIMPLE_SELF_ATT = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
                         att_type = 'simple_self_att', emb_dim = 100, lr = 5e-4, epochs = 10)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat', emb_dim = 100, lr = 1e-4, epochs = 20,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 550,
     num_head = 7)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat', emb_dim = 100, lr = 5e-4, epochs = 20,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     num_head = 3)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat', emb_dim = 100, lr = 5e-4, epochs = 20,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     num_head = 1)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat', emb_dim = 100, lr = 5e-4, epochs = 20,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     num_head = 5) #sparse, adagrad

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat', emb_dim = 100, lr = 5e-3, epochs = 20,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     num_head = 5) #sparse, adagrad

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat', emb_dim = 100, lr = 5e-2, epochs = 20,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     num_head = 5) #sparse, adagrad

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat', emb_dim = 100, lr = 1e-2, epochs = 20,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     num_head = 5) #sparse, adagrad

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat', emb_dim = 100, lr = 5e-3, epochs = 20,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     num_head = 3) #sparse, adagrad

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat', emb_dim = 100, lr = 5e-3, epochs = 10,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     num_head = 3)

print('top 30 words with highest std / mean')
print(att_prob.get_stat(30))

MODEL_ATT_WEIGHTED = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
                 att_type = 'att_weighted', emb_dim = 100, lr = 5e-4, epochs = 5, seed = 123)

MODEL_ATT_WEIGHTED = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
                 att_type = 'att_weighted', emb_dim = 100, lr = 5e-4, epochs = 5, seed = 125)

MODEL_SIMPLE_SELF_ATT = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
    att_type = 'simple_self_att', emb_dim = 100, lr = 5e-4, epochs = 10, seed = 438)

MODEL_SIMPLE_SELF_ATT = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
    att_type = 'simple_self_att', emb_dim = 100, lr = 1e-4, epochs = 20, seed = 553)

MODEL_SIMPLE_SELF_ATT = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
    att_type = 'simple_self_att', emb_dim = 100, lr = 5e-4, epochs = 20, seed = 438)

MODEL_SIMPLE_SELF_ATT = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
    att_type = 'simple_self_att', emb_dim = 100, lr = 5e-3, epochs = 5, seed = 438)

MODEL_SIMPLE_SELF_ATT2 = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
    att_type = 'simple_self_att', emb_dim = 500, lr = 5e-4, epochs = 10, seed = 438)

MODEL_SIMPLE_SELF_ATT2 = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
    att_type = 'simple_self_att', emb_dim = 1000, lr = 5e-3, epochs = 10, seed = 438)

MODEL_NONE = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
                  att_type = None, emb_dim = 100, lr = 1e-2, epochs = 5, seed = 1)

MODEL_NONE = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
                  att_type = None, emb_dim = 100, lr = 1e-2, epochs = 15, seed = 5)

MODEL_NONE = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
                  att_type = None, emb_dim = 100, lr = 1e-3, epochs = 15, seed = 6)

"""## enriching results"""

m = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
         att_type = 'self_att', emb_dim = 100, lr = 5e-4, epochs = 10,
         try_gpu = True, verbose = True, plot = True, save_best = False, seed = 555,
         att_dim = 64)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
         att_type = 'self_att', emb_dim = 100, lr = 5e-3, epochs = 5,
         try_gpu = True, verbose = True, plot = True, save_best = True, seed = 556,
         att_dim = 64)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
         att_type = 'self_att', emb_dim = 100, lr = 5e-3, epochs = 5,
         try_gpu = True, verbose = True, plot = True, save_best = True, seed = 556,
         att_dim = 64)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_sum', emb_dim = 100, lr = 5e-3, epochs = 5, 
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     att_dim = 64, num_head = 5)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_sum', emb_dim = 100, lr = 1e-3, epochs = 5, 
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     att_dim = 64, num_head = 5)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821,  # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_sum', emb_dim = 512, lr = 5e-4, epochs = 5, 
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     num_head = 8)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat', emb_dim = 100, lr = 5e-4, epochs = 5,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     num_head = 5)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat', emb_dim = 100, lr = 5e-4, epochs = 15,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 558,
     num_head = 3)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat', emb_dim = 100, lr = 5e-4, epochs = 15,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 558,
     num_head = 7)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat', emb_dim = 100, lr = 1e-5, epochs = 20,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     num_head = 5)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_sum', emb_dim = 100, lr = 5e-4, epochs = 15,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 556,
     num_head = 5)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_sum', emb_dim = 100, lr = 5e-4, epochs = 15,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 560,
     num_head = 10)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat_pos', emb_dim = 100, lr = 5e-4, epochs = 20,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     num_head = 5)

m = eval(train_size = 67349, dev_size = 872, test_size = 1821, # max: 67349, 872, 1821
     att_type = 'self_att_multi_head_cat_pos', emb_dim = 100, lr = 5e-3, epochs = 10,
     try_gpu = True, verbose = True, plot = True, save_best = True, seed = 555,
     num_head = 5)